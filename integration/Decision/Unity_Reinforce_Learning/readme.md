# 개요

만약 지금 강화학습 결과를 보고 싶다면 Assets(3).zip 파일 내부 파일을
\ml-agents\UnitySDK\Assets 에 덮어쓰기 하면 됨.


학습하는 모습을 보고 싶으면
http://165.132.138.176:6006 tensorboard 에서 확인 가능. 다만 언제든지 끌 수도 있음.

# 학습 결과 정리
각 학습 상황은 위 텐서 보드에서 정확하게 확인할 수 있음.


## 1~5
11 방향 LiDAR에, 적 위치와 자신의 위치/방향/속도를 등 55개의 input Vector를 주고, 전/정지/후 3벡터, 좌/정지/우 3벡터, 시계/정지/반시계 3벡터, 발사/정지 2벡터로 총 11개의 Action Vector로 구성됨. 

 * 1,2번 실험은 기록 진행하지 않음
 * 3번 실험 : 55개 벡터와, 명중 시 50 리워드, 불발/피격/벽충둘 시 -1 리워드로 100 step 학습, 활발하게 움직이나 명중하지 못함.
 * 4번 실험 : 학습 에이전트 수를 늘렸으나 서로 충돌나서 오히려 좋지 않은 결과만 나옴.
 * 5번 실험 : 명중 리워드를 5로 줄이고 진행, 가만히 있는게 낫다고 학습됨.
 
 ## 6~10
 18 방향 LiDAR에,적 위치와 자신의 위치/방향/속도 등 83개 input Vector를 주고, 좌/정지/우 3벡터, 시계/정지/반시계 3벡터, 발사/정지 2벡터로 총 11개의 Action Vector로 구성됨.
 
 * 6번 실험 : 50만 step 학습, 3번과 비슷한 결과.
 * 7번 실험 : 불발 패널티를 늘림, 움직이지 않고 발사도 안하게 학습됨.
 * 8번 실험 : 500만 step 학습, 명중 리워드를 조정하여 학습 재 진행. 어느정도 싸우는 느낌이 나게 학습됨 동영상으로 사용함.
 * 9번 실험 : 승리 리워드 도입/벽 충돌 리워드 삭제. 좋은 결과는 안 나옴.
 * 10번 실험 : 멈춤 명령 추가, 큰 차이 없음.
 *2대 2 실험 : 위 조건과 동일하나 2대2 개념을 도입, 아군과 적군 개념과 시체 개념을 추가. 10 여번에 실험을 하였으나 쏘지 않는 방향으로 학습이 진행되어 버림.
 ## 11~
 실험 환경을 실제 경기환경과 동일하게 변경(시간, 보너스, 방어, 잔탄)하고, 발사 명령을 주는 heat-scan 방식으로 변경.
 
 * 11번 실험 : 방어 보너스 개념 적용
 * 12번 실험 : 재장전 개념 적용
 * 13번 실험 : 대치(Confront) 적용. 서로 마주 볼 경우 리워드 적용 시간 제한을 두지 않아서 리워드가 발산.
 * 14번 실험 : 대치 리워드에 시간 제한(1초에 한 번)을 주고 실험. 대치를 하나 여전히 서로를 보지 못함.
 * 15번 실험 : 자잘한 리워드 수정
 * 16번 실험 : 대치 리워드와 총알 개수 연관.
 * 17번 실험 : 방어 보너스가 없을 경우 보너스 존에 서있을 때 리워드 적용.
 
 
